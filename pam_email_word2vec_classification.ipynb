{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQfMYXPFWGBK",
        "outputId": "39a80af6-7939-4a74-cff8-b94ac142f087"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (5572, 2)\n",
            "Train size: 4457 Test size: 1115\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "df = pd.read_csv(\"spam.csv\")\n",
        "print(\"Data shape:\", df.shape)\n",
        "df.head()\n",
        "\n",
        "texts = df[\"text\"].astype(str).tolist()\n",
        "labels = df[\"target\"].tolist()\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "labels_encoded = label_encoder.fit_transform(labels)\n",
        "\n",
        "X_train_texts, X_test_texts, y_train, y_test = train_test_split(\n",
        "    texts, labels_encoded, test_size=0.2, random_state=42, stratify=labels_encoded\n",
        ")\n",
        "\n",
        "print(\"Train size:\", len(X_train_texts), \"Test size:\", len(X_test_texts))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOw6yyfiX4VS",
        "outputId": "dfddfa22-a950-4ee3-b8a7-5c73861e9fb5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Tokenize texts\n",
        "def tokenize(text):\n",
        "    return nltk.word_tokenize(text.lower())\n",
        "\n",
        "tokenized_train = [tokenize(text) for text in X_train_texts]\n",
        "tokenized_test = [tokenize(text) for text in X_test_texts]\n",
        "\n",
        "# Train Word2Vec on your training texts\n",
        "w2v_model = Word2Vec(sentences=tokenized_train, vector_size=100, window=5, min_count=1, workers=4)\n",
        "print(\"Word2Vec vocab size:\", len(w2v_model.wv))\n",
        "\n",
        "# Function to average word vectors for each text\n",
        "def text_to_vector(tokens):\n",
        "    vectors = [w2v_model.wv[word] for word in tokens if word in w2v_model.wv]\n",
        "    if vectors:\n",
        "        return np.mean(vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(100)\n",
        "\n",
        "X_train_vec = np.array([text_to_vector(tokens) for tokens in tokenized_train])\n",
        "X_test_vec = np.array([text_to_vector(tokens) for tokens in tokenized_test])\n",
        "\n",
        "print(\"Train vector shape:\", X_train_vec.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTX8H_HOX91B",
        "outputId": "35f04634-fa82-4697-9df4-e7fcf990cb33"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec vocab size: 8384\n",
            "Train vector shape: (4457, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Naive Bayes\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train_vec, y_train)\n",
        "y_pred_nb = nb.predict(X_test_vec)\n",
        "print(\"Naive Bayes Results:\\n\", classification_report(y_test, y_pred_nb))\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train_vec, y_train)\n",
        "y_pred_rf = rf.predict(X_test_vec)\n",
        "print(\"Random Forest Results:\\n\", classification_report(y_test, y_pred_rf))\n",
        "\n",
        "# XGBoost\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
        "xgb.fit(X_train_vec, y_train)\n",
        "y_pred_xgb = xgb.predict(X_test_vec)\n",
        "print(\"XGBoost Results:\\n\", classification_report(y_test, y_pred_xgb))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHzzlWBNaqbX",
        "outputId": "9d252d56-6232-43ea-88fa-4443350c1fb0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Results:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.68      0.79       966\n",
            "           1       0.26      0.74      0.39       149\n",
            "\n",
            "    accuracy                           0.69      1115\n",
            "   macro avg       0.60      0.71      0.59      1115\n",
            "weighted avg       0.85      0.69      0.74      1115\n",
            "\n",
            "Random Forest Results:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.98      0.96       966\n",
            "           1       0.83      0.57      0.68       149\n",
            "\n",
            "    accuracy                           0.93      1115\n",
            "   macro avg       0.89      0.78      0.82      1115\n",
            "weighted avg       0.92      0.93      0.92      1115\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [14:36:16] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGBoost Results:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.98      0.97       966\n",
            "           1       0.87      0.70      0.77       149\n",
            "\n",
            "    accuracy                           0.95      1115\n",
            "   macro avg       0.91      0.84      0.87      1115\n",
            "weighted avg       0.94      0.95      0.94      1115\n",
            "\n"
          ]
        }
      ]
    }
  ]
}